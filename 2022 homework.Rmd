---
title: "Rethinking statistics 2022 homework"
author: "Derek Murphy"
date: "02/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rethinking)
library(knitr)
library(dagitty)

```

## Statistical Rethinking 2022

Homework for Richard McElreath's Statistical Rethinking 2022 course.
Lectures available on youtube [here](https://www.youtube.com/watch?v=cclUd_HoRlo&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN) and homework and other resources available on github [here](https://github.com/rmcelreath/stat_rethinking_2022)


## Week 1 homework
Covering chapters 1, 2 and 3 of the book.

### Q1
Suppose the globe tossing data (Chapter 2) had turned out to be 4 water
and 11 land. Construct the posterior distribution, using grid approximation.
Use the same flat prior as in the book.

```{r w1.1}
#set variables
W <- 4
L <- 11

#define grid
pgrid <- seq(0, 1, length.out = 1000)

#define flat prior
prior <- rep(1, 20)

#calculate likelihood at each level of probability in the grid
likelihood <- dbinom(W, size = W+L, prob = pgrid)

#calculate product of likelihood and prior
unstd.posterior <- prior * likelihood

#standardise the posterior so it sums to 1
posterior <- unstd.posterior/sum(unstd.posterior)

#plot the posterior
plot(pgrid, posterior, type = 'l', xlab = "Proportion of water", ylab = "Posterior probability")
```

### Q2 
Now suppose the data are 4 water and 2 land. Compute the posterior
again, but this time use a prior that is zero below p = 0.5 and a constant
above p = 0.5. This corresponds to prior information that a majority of the
Earth’s surface is water.

```{r w1.2}
W <- 4
L <- 2

pgrid <- seq(0, 1, length.out = 1000)

prior <- ifelse(pgrid <= 0.5, 0, 1)

likelihood <- dbinom(W, size = W+L, prob = pgrid)

unstd.posterior <- prior*likelihood

posterior <- unstd.posterior/sum(unstd.posterior)


plot(pgrid, posterior, type = 'l', xlab = "proportion of water", ylab = "posterior probability")
```

### Q3
For the posterior distribution from 2, compute 89% percentile and HPDI (Highest Posterior Density Interval)
intervals. Compare the widths of these intervals. 

```{r w1.3}
#Compute the intervals based on samples from the posterior
set.seed(100)
samps <- sample(pgrid, size = 1e4, replace = TRUE, prob = posterior)

pi89  <- rethinking::PI(samps, prob = 0.89) #same as: quantile(samps, probs = c(0.055, 0.945))
hpdi<-rethinking::HPDI(samps)

pi89 #89% percentile

hpdi #HPDI (89%)

```

Which is wider?  
*the 89% quantile (`r pi89[[1]]` - `r pi89[[2]]`) is wider than the HPDI (`r hpdi[[1]]` - `r hpdi[[2]]`)*

Why?  
*Because the HPDI is the narrowest interval containing the 89% probability mass. It can differ a lot from the 89% quantile if the posterior distribution is skewed, but in this case they are not very different.*

If you had only the information in the interval, what might you misunderstand
about the shape of the posterior distribution?  
*You might assume that there is some probability that the model assigns to a world with <50% water coverage, when in fact the posterior distribution gives no probability of a world with <50% water coverage.*



### Q4. OPTIONAL CHALLENGE. 
Suppose there is bias in sampling so that Land
is more likely than Water to be recorded. Specifically, assume that 1-in-5
(20%) of Water samples are accidentally recorded instead as ”Land”. First,
write a generative simulation of this sampling process. Assuming the true
proportion of Water is 0.70, what proportion does your simulation tend to
produce instead? 

```{r w1.4}

#first write a function that returns 1 for observing water and 0 for land
globe_toss <- function(n_toss = 100, prob_w = 0.7){
  x <- rbinom(n_toss, 1, prob_w)
  x <- replace(x, x==1, rbinom(length(x[x==1]), 1, 0.8))

  return(x)
}


#The simulation produces the following proportion of water:
set.seed(100)
sum(globe_toss(n_toss = 1e4))/1e4

```

Second, using a simulated sample of 20 tosses, compute
the unbiased posterior distribution of the true proportion of water.  
*I presume we know what the sampling bias is in this example?*

```{r w4.1.2}
set.seed(100)
tosses <- globe_toss(n_toss = 20)

pgrid <- seq(0, 1, length.out = 100)

prior <- rep(1, 20)

#To get the unbiased likelihood, multiply pgrid by 0.8 
#because we know true probability of finding water is 0.7(ground truth) * 0.8(sampling bias)
likelihood <- dbinom(sum(tosses), size = length(tosses), prob = pgrid*0.8) 

unstd.posterior <- prior*likelihood

posterior <- unstd.posterior/sum(unstd.posterior)

plot(pgrid, posterior, type = 'l', xlab = "proportion of water", ylab = 'posterior probability')

```

Can you figure out that simulation and model as well, in which both
water and land have chances of being misclassified?

**_the following isn't right, but I can't figure it out :(_**

```{r w1.4.3}
#let's say land has a 10% chance of being misclassified as water, 
#as well as water having a 20% chance of being misclassified as land.
set.seed(100)

n<-1e3

#the generative simulation applied in the globe_toss function above can be written in one line:
obs <- rbinom(n, 20, prob = 0.7*0.8)
#This says the probability of recording an observation of water on any toss is equal to 
#the true proportion of water (0.7) * the chance of correctly classifying water (0.8).
# "If the are 0.7 out of 1 ways to sample water and
# 0.8 out of 1 ways for water to be reported as water, then there must be 0.7 ×
# 0.8 ways to observe water
#So in this example, there are 0.7*0.8 ways of classifying water when water is observed, and
#there are 1-0.7 ways of observing land but 0.1*(1-0.7) ways of classifying land as water, so
obs <- rbinom(n, 20, prob = (0.7*0.8 + 0.3*0.1)) #?

#The proportion of observed water in this new simulation is:
mean(obs/20)

#Now to model this in an unbiased way:
W<-rbinom(1, 20, prob = (0.7*0.8 + 0.1*(1-0.7)))
pgrid <- seq(0, 1, length.out = 100)
#prior <- rep(1, 100)
prior<-dbeta(pgrid, 1, 1)
likelihood <- dbinom(W, 20, prob = (pgrid*0.8 + 0.1*(1-pgrid)))
unstd.posterior <- prior*likelihood
posterior <- unstd.posterior/sum(unstd.posterior)

wrong_likelihood <- dbinom(W, 20, prob = (pgrid*0.8))
wrong_unstd.posterior <- prior*wrong_likelihood
wrong_posterior <- wrong_unstd.posterior/sum(wrong_unstd.posterior)

plot(pgrid, posterior, type = 'l', xlab = 'proportion of water', ylab = 'posterior probability')
lines(pgrid, wrong_posterior, col = "red")
legend("topleft", c("unbiased", "biased (not accounting for land misclass)"),
       lty = c(1,1),
       col = c("black", "red"))
```


*By not accounting for the measurement error that misclassifies land as water 10% of the time, the biased posterior estimate gives greater probability to higher proportions of water than the unbiased posterior*


## Week 2 homework
Covering chapters 4 and 5 of the book.

### Q1
Construct a linear regression of weight as predicted by height, using the
adults (age 18 or greater) from the Howell1 dataset. The heights listed below
were recorded in the !Kung census, but weights were not recorded for these
individuals. Provide predicted weights and 89% compatibility intervals for
each of these individuals. That is, fill in the table below, using model-based
predictions.

```{r q1 table}

individual <- c(1, 2, 3)
height <- c(140, 160, 175)
tabledf <- data.frame(individual = individual, height = height, expected_weight = NA)

kable(tabledf)
```

First, define the model:

```{r w2.1}
data("Howell1")
df<-Howell1 %>% filter(age >= 18) %>% mutate(H = standardize(height), W = standardize(weight))

m2.1 <- quap(
  flist <- alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b*H,
    a ~ dnorm(0, 1),
    b ~ dlnorm(0, 1),
    sigma ~ dexp(1)
    ),
  data = df
)

```

Next, simulate the prior predictive distribution of regression lines

```{r w2.1.2}
prior <- extract.prior( m2.1 )
xseq <- c(-2,2)
mu <- link( m2.1 , post=prior , data=list(H=xseq) )
plot( NULL , xlim=xseq , ylim=xseq, xlab = 'std.height', ylab = 'std.weight' )
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )
```

There are some unlikely lines, suggesting enormous increases in weight with height, but that should be ok for the model and data.

Now plot the posterior predictive distribution (including the average regression line and 89% compatability interval, and the 89% prediction interval)

```{r w2.1.3}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
H.seq <- seq( from=-3 , to=3 , length.out = 100 )

# use link to compute mu
# for each sample from posterior
# and for each height in H.seq
mu <- link( m2.1 , data=data.frame(H=H.seq) )

# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

#convert standardized weight and height back to natural scale?
nat.H.seq <- H.seq*sd(df$height) + mean(df$height)
nat.mu.mean <- mu.mean*sd(df$weight) + mean(df$weight)
nat.mu.PI <- mu.PI*sd(df$weight) + mean(df$weight)

# plot raw data
# fading out points to make line and interval more visible
plot( weight ~ height , data=df , col=col.alpha(rangi2,0.5) )

# plot the quap line, aka the mean mu for each weight
lines( nat.H.seq , nat.mu.mean )

# plot a shaded region for 89% PI
shade( nat.mu.PI , nat.H.seq )

# plot the prediction interval (values the model expects)
sim.W <- sim( m2.1 , data=list(H=H.seq) ) 
W.PI <- apply( sim.W , 2 , PI , prob=0.89 )
#convert to natural scale
nat.W.PI <- W.PI*sd(df$weight) + mean(df$weight)

# draw PI region for simulated heights
shade( nat.W.PI , nat.H.seq )



```

Now complete the table

```{r w2.1.4}
#extract mean and 89% predicted values of the posterior distribution to complete the table:
#post<-extract.samples(m4h1)
tabledf <- tabledf %>% mutate(H = (height-mean(df$height))/sd(df$height))
sim_weight<-sim(m2.1, data = tabledf)
tabledf$expected_weight<-apply(sim_weight, 2, mean)*sd(df$weight) + mean(df$weight)
tabledf$lower_interval<-apply(sim_weight, 2, PI, prob = 0.89)[1,]*sd(df$weight) + mean(df$weight)
tabledf$upper_interval<-apply(sim_weight, 2, PI, prob = 0.89)[2,]*sd(df$weight) + mean(df$weight)

kable(tabledf %>% select(-"H"))

```


### Q2  
From the Howell1 dataset, consider only the people younger than 13 years
old. Estimate the causal association between age and weight. Assume that
age influences weight through two paths. First, age influences height, and
height influences weight. Second, age directly influences weight through age-
related changes in muscle growth and body proportions. All of this implies
this causal model (DAG):

```{r w2.2.dag, fig.dim=c(2,1.5)}

g <- dagitty::dagitty( "dag{
  A -> H; A -> W; H -> W
}"
)
dagitty::coordinates(g) <- list(x = c(A = 1, H = 2, W = 3), y = c(A = 1, H = -2, W = 1))
drawdag(g, cex = 1.5, lwd = 3)
```

Use a linear regression to estimate the total (not just direct) causal effect of
each year of growth on weight. Be sure to carefully consider the priors. Try
using prior predictive simulation to assess what they imply.

```{r w2.2}
#load the data and define the model

data("Howell1")
df_kids<-Howell1 %>% filter(age<13)
mean_age = mean(df_kids$age)

#For the total effect of age on weight, no need to include height in the model

m2.2 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b*age,
    a ~ dnorm(3, 1), #mean birth weight is roughly 3kg (wiki) with a small variation
    b ~ dlnorm(0, 1), #weight (typically) increases with age so b must be +ve (still fuzzy on lognorm values)
    sigma ~ dexp(1)
  ), data = df_kids
)

```

Use prior predictive simulation to assess the priors.

```{r w2.2.2}
prior <- extract.prior( m2.2 )
xseq <- 0:12
mu <- link( m2.2 , post=prior , data=list(age = 0:12) )
plot( NULL , xlim=c(0, 12) , ylim=c(0, 30), xlab = 'age', ylab = 'weight' )
for ( i in 1:50 ) lines( 0:12 , mu[i,] , col=col.alpha("black",0.3) )

```

This allows a wide range of relationships between age and weight - some are unreasonably strong, but the data should be able to deal with this.

Let's look at the posterior distribution and model predictions:

```{r w2.2.3}
# define sequence of ages to compute predictions for
# these values will be on the horizontal axis
age_seq <- 0:12

# use link to compute mu
# for each sample from posterior
# and for each height in H.seq
mu <- link( m2.2 , data=data.frame(age=age_seq) )

# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )


# plot raw data
# fading out points to make line and interval more visible
plot( weight ~ age , data=df_kids , col=col.alpha(rangi2,0.5) )

# plot the quap line, aka the mean mu for each weight
lines( age_seq , mu.mean )

# plot a shaded region for 89% PI
shade( mu.PI , age_seq )

# plot the prediction interval (values the model expects)
sim_w <- sim( m2.2 , data=list(age = age_seq) ) 
w_pi <- apply( sim_w , 2 , PI , prob=0.89 )

# draw PI region for simulated heights
shade( w_pi , age_seq )

#plot the coefficient estimates
#coeftab_plot(coeftab(m2.2))

precis(m2.2)

```

Mean birth weight is expected to be about 7kg, with weight increasing by 1.3-1.5kg per year.


### Q3.  

Now suppose the causal association between age and weight might be dif-
ferent for boys and girls. Use a single linear regression, with a categorical
variable for sex, to estimate the total causal effect of age on weight separately
for boys and girls. How do girls and boys differ? Provide one or more pos-
terior contrasts as a summary.

```{r w2.3}
#create sex column in df, where female == 1 and male == 2
df_kids <- df_kids %>% mutate(sex = male+1)

#define the model

m2.3 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma),
    mu <- a[sex] + b[sex]*age,
    a[sex] ~ dnorm(3, 1),
    b[sex] ~ dlnorm(0, 1),
    sigma ~ dexp(1)
    ),
  data = df_kids
)

#print model coeficients
precis(m2.3, depth = 2)

#calculate posterior contrasts
post <- extract.samples(m2.3)
diff_sex <- post$a[,1] - post$a[,2]
PI( diff_sex )

#plot contrasts
age_seq <- 0:12
# compute mu over samples, fixing sex at female
mu_f <- link( m2.3, data=data.frame( sex=1 , age=age_seq ) )

# compute mu over samples, fixing sex at male
mu_m <- link( m2.3, data=data.frame( sex=2 ,  age=age_seq ) )

# summarize to means and intervals
mu_f_mu <- apply( mu_f , 2 , mean )
mu_f_ci <- apply( mu_f , 2 , PI , prob=0.97 )
mu_m_mu <- apply( mu_m , 2 , mean )
mu_m_ci <- apply( mu_m , 2 , PI , prob=0.97 )

#plot the data
cols <- c("red", "blue")
plot(weight ~ age, data = df_kids, col = cols[sex])

# plot the quap line, aka the mean mu for each weight
lines( age_seq , mu_f_mu, col = cols[1], lwd = 3 )
lines( age_seq , mu_m_mu, col = cols[2], lwd = 3 )

# plot a shaded region for 89% PI
shade( mu_f_ci , age_seq, col = col.alpha("red", 0.5))
shade( mu_m_ci , age_seq, col = col.alpha("blue", 0.5))

legend("topleft", c("female", "male"),
       lty = c(1,1),
       col = c("red", "blue"))

```

Boys are typically heavier than girls of the same age, and it looks like the difference increases with age.

BUt let's contrast the predicted distribution of weights across all ages to get a better idea of what's going on.

```{r w2.3.3}
# contrast at each age
Aseq <- 0:12
mu1 <- sim(m2.3,data=list(age=Aseq,sex=rep(1,13)))
mu2 <- sim(m2.3,data=list(age=Aseq,sex=rep(2,13)))
mu_contrast <- mu1
for ( i in 1:13 ) mu_contrast[,i] <- mu2[,i] - mu1[,i]
plot( NULL , xlim=c(0,13) , ylim=c(-15,15) , xlab="age" ,
ylab="girls heavier <- weight difference -> boys heavier" )
for ( p in c(0.5,0.67,0.89,0.99) )
shade( apply(mu_contrast,2,PI,prob=p) , Aseq )
abline(h=0,lty=2,lwd=2)

```

Boys tend to be heavier across all ages, but the distributions overlap a lot, particularly for younger kids. In older kids, the difference between boys' and girls' weight gets stronger.


### Q4  OPTIONAL CHALLENGE. 
The data in data(Oxboys) (rethinking
package) are growth records for 26 boys measured over 9 periods. I want
you to model their growth. Specifically, model the increments in growth
from one period (Occasion in the data table) to the next. Each increment is
simply the difference between height in one occasion and height in the pre-
vious occasion. Since none of these boys shrunk during the study, all of the
growth increments are greater than zero. Estimate the posterior distribution
of these increments. Constrain the distribution so it is always positive—it
should not be possible for the model to think that boys can shrink from year
to year. Finally compute the posterior distribution of the total growth over
all 9 occasions.



```{r w2.4}
data("Oxboys")

#as we haven't been taught how to use random effects in a quap model yet, 
#I presume we ignore the random effect of ID?

#calculate growth increments in df
df <- Oxboys %>% group_by(Subject) %>%
  mutate(increment = case_when(
    Occasion==1 ~ 0,
    TRUE ~ height-lag(height)
  )) %>% ungroup() %>%
  filter(Occasion > 1)  #no need to model increment at Occasion 1 as this is always 0

#define the model:
m2.4 <- quap(
  flist = alist(
    increment ~ dlnorm(alpha, sigma), #use lognormal dist to constrain increments to be positive
    alpha ~ dnorm(0, 0.1), # alpha is the mean of the normal distribution, whose log we use for increment
    sigma ~ dexp(3)
  ),
  data = df
)

#plot prior predictive distribution of increments
prior <- extract.prior( m2.4, n = 1e3 )
sim_increment <- rlnorm(prior$alpha, prior$sigma)
dens(sim_increment)
#The code below also does the same thing (I think)
# simulation from priors
n <- 1e3
alpha <- rnorm(n,0,0.1)
sigma <- rexp(n,3)
delta_sim <- rlnorm(n,alpha,sigma)
dens(delta_sim)
#Play around with values for alpha and sigma - difficult to understand how they will affect the distribution!


#Now plot the distribution of increments from the posterior
post <- extract.samples(m2.4)
dsim <- rlnorm(1e3, post$alpha,post$sigma)
dens(dsim)

#and to get the distribution of total growth over the 9 occasions:
#(what is this doing?
#we're generating 8 values from a lognormal distribution with the same values of alpha and sigma,
#then summing them, and we're doing that for the first 1000 rows from the posterior sample)
inc_sum <- sapply( 1:1000 ,
                   function(s) sum(rlnorm(8,post$alpha[s],post$sigma[s])) )
dens(inc_sum)

```