---
title: "Rethinking statistics 2022 homework"
author: "Derek Murphy"
date: "02/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rethinking)
library(knitr)
library(dagitty)
library(cmdstanr)

```

## Statistical Rethinking 2022

Homework for Richard McElreath's Statistical Rethinking 2022 course.
Lectures available on youtube [here](https://www.youtube.com/watch?v=cclUd_HoRlo&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN) and homework and other resources available on github [here](https://github.com/rmcelreath/stat_rethinking_2022)


## Week 1 homework
Covering chapters 1, 2 and 3 of the book.

### Q1
Suppose the globe tossing data (Chapter 2) had turned out to be 4 water
and 11 land. Construct the posterior distribution, using grid approximation.
Use the same flat prior as in the book.

```{r w1.1}
#set variables
W <- 4
L <- 11

#define grid
pgrid <- seq(0, 1, length.out = 1000)

#define flat prior
prior <- rep(1, 20)

#calculate likelihood at each level of probability in the grid
likelihood <- dbinom(W, size = W+L, prob = pgrid)

#calculate product of likelihood and prior
unstd.posterior <- prior * likelihood

#standardise the posterior so it sums to 1
posterior <- unstd.posterior/sum(unstd.posterior)

#plot the posterior
plot(pgrid, posterior, type = 'l', xlab = "Proportion of water", ylab = "Posterior probability")
```

### Q2 
Now suppose the data are 4 water and 2 land. Compute the posterior
again, but this time use a prior that is zero below p = 0.5 and a constant
above p = 0.5. This corresponds to prior information that a majority of the
Earth’s surface is water.

```{r w1.2}
W <- 4
L <- 2

pgrid <- seq(0, 1, length.out = 1000)

prior <- ifelse(pgrid <= 0.5, 0, 1)

likelihood <- dbinom(W, size = W+L, prob = pgrid)

unstd.posterior <- prior*likelihood

posterior <- unstd.posterior/sum(unstd.posterior)


plot(pgrid, posterior, type = 'l', xlab = "proportion of water", ylab = "posterior probability")
```

### Q3
For the posterior distribution from 2, compute 89% percentile and HPDI (Highest Posterior Density Interval)
intervals. Compare the widths of these intervals. 

```{r w1.3}
#Compute the intervals based on samples from the posterior
set.seed(100)
samps <- sample(pgrid, size = 1e4, replace = TRUE, prob = posterior)

pi89  <- rethinking::PI(samps, prob = 0.89) #same as: quantile(samps, probs = c(0.055, 0.945))
hpdi<-rethinking::HPDI(samps)

pi89 #89% percentile

hpdi #HPDI (89%)

```

Which is wider?  
*the 89% quantile (`r pi89[[1]]` - `r pi89[[2]]`) is wider than the HPDI (`r hpdi[[1]]` - `r hpdi[[2]]`)*

Why?  
*Because the HPDI is the narrowest interval containing the 89% probability mass. It can differ a lot from the 89% quantile if the posterior distribution is skewed, but in this case they are not very different.*

If you had only the information in the interval, what might you misunderstand
about the shape of the posterior distribution?  
*You might assume that there is some probability that the model assigns to a world with <50% water coverage, when in fact the posterior distribution gives no probability of a world with <50% water coverage.*



### Q4. OPTIONAL CHALLENGE. 
Suppose there is bias in sampling so that Land
is more likely than Water to be recorded. Specifically, assume that 1-in-5
(20%) of Water samples are accidentally recorded instead as ”Land”. First,
write a generative simulation of this sampling process. Assuming the true
proportion of Water is 0.70, what proportion does your simulation tend to
produce instead? 

```{r w1.4}

#first write a function that returns 1 for observing water and 0 for land
globe_toss <- function(n_toss = 100, prob_w = 0.7){
  x <- rbinom(n_toss, 1, prob_w)
  x <- replace(x, x==1, rbinom(length(x[x==1]), 1, 0.8))

  return(x)
}


#The simulation produces the following proportion of water:
set.seed(100)
sum(globe_toss(n_toss = 1e4))/1e4

```

Second, using a simulated sample of 20 tosses, compute
the unbiased posterior distribution of the true proportion of water.  
*I presume we know what the sampling bias is in this example?*

```{r w4.1.2}
set.seed(100)
tosses <- globe_toss(n_toss = 20)

pgrid <- seq(0, 1, length.out = 100)

prior <- rep(1, 20)

#To get the unbiased likelihood, multiply pgrid by 0.8 
#because we know true probability of finding water is 0.7(ground truth) * 0.8(sampling bias)
likelihood <- dbinom(sum(tosses), size = length(tosses), prob = pgrid*0.8) 

unstd.posterior <- prior*likelihood

posterior <- unstd.posterior/sum(unstd.posterior)

plot(pgrid, posterior, type = 'l', xlab = "proportion of water", ylab = 'posterior probability')

```

Can you figure out that simulation and model as well, in which both
water and land have chances of being misclassified?

**_the following isn't right, but I can't figure it out :(_**

```{r w1.4.3}
#let's say land has a 10% chance of being misclassified as water, 
#as well as water having a 20% chance of being misclassified as land.
set.seed(100)

n<-1e3

#the generative simulation applied in the globe_toss function above can be written in one line:
obs <- rbinom(n, 20, prob = 0.7*0.8)
#This says the probability of recording an observation of water on any toss is equal to 
#the true proportion of water (0.7) * the chance of correctly classifying water (0.8).
# "If the are 0.7 out of 1 ways to sample water and
# 0.8 out of 1 ways for water to be reported as water, then there must be 0.7 ×
# 0.8 ways to observe water
#So in this example, there are 0.7*0.8 ways of classifying water when water is observed, and
#there are 1-0.7 ways of observing land but 0.1*(1-0.7) ways of classifying land as water, so
obs <- rbinom(n, 20, prob = (0.7*0.8 + 0.3*0.1)) #?

#The proportion of observed water in this new simulation is:
mean(obs/20)

#Now to model this in an unbiased way:
W<-rbinom(1, 20, prob = (0.7*0.8 + 0.1*(1-0.7)))
pgrid <- seq(0, 1, length.out = 100)
#prior <- rep(1, 100)
prior<-dbeta(pgrid, 1, 1)
likelihood <- dbinom(W, 20, prob = (pgrid*0.8 + 0.1*(1-pgrid)))
unstd.posterior <- prior*likelihood
posterior <- unstd.posterior/sum(unstd.posterior)

wrong_likelihood <- dbinom(W, 20, prob = (pgrid*0.8))
wrong_unstd.posterior <- prior*wrong_likelihood
wrong_posterior <- wrong_unstd.posterior/sum(wrong_unstd.posterior)

plot(pgrid, posterior, type = 'l', xlab = 'proportion of water', ylab = 'posterior probability')
lines(pgrid, wrong_posterior, col = "red")
legend("topleft", c("unbiased", "biased (not accounting for land misclass)"),
       lty = c(1,1),
       col = c("black", "red"))
```


*By not accounting for the measurement error that misclassifies land as water 10% of the time, the biased posterior estimate gives greater probability to higher proportions of water than the unbiased posterior*


## Week 2 homework
Covering chapters 4 and 5 of the book (lectures 3 and 4).

### Q1
Construct a linear regression of weight as predicted by height, using the
adults (age 18 or greater) from the Howell1 dataset. The heights listed below
were recorded in the !Kung census, but weights were not recorded for these
individuals. Provide predicted weights and 89% compatibility intervals for
each of these individuals. That is, fill in the table below, using model-based
predictions.

```{r q1 table}

individual <- c(1, 2, 3)
height <- c(140, 160, 175)
tabledf <- data.frame(individual = individual, height = height, expected_weight = NA)

kable(tabledf)
```

First, define the model:

```{r w2.1}
data("Howell1")
df<-Howell1 %>% filter(age >= 18) %>% mutate(H = standardize(height), W = standardize(weight))

m2.1 <- quap(
  flist <- alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b*H,
    a ~ dnorm(0, 1),
    b ~ dlnorm(0, 1),
    sigma ~ dexp(1)
    ),
  data = df
)

```

Next, simulate the prior predictive distribution of regression lines

```{r w2.1.2}
prior <- extract.prior( m2.1 )
xseq <- c(-2,2)
mu <- link( m2.1 , post=prior , data=list(H=xseq) )
plot( NULL , xlim=xseq , ylim=xseq, xlab = 'std.height', ylab = 'std.weight' )
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )
```

There are some unlikely lines, suggesting enormous increases in weight with height, but that should be ok for the model and data.

Now plot the posterior predictive distribution (including the average regression line and 89% compatability interval, and the 89% prediction interval)

```{r w2.1.3}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
H.seq <- seq( from=-3 , to=3 , length.out = 100 )

# use link to compute mu
# for each sample from posterior
# and for each height in H.seq
mu <- link( m2.1 , data=data.frame(H=H.seq) )

# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

#convert standardized weight and height back to natural scale?
nat.H.seq <- H.seq*sd(df$height) + mean(df$height)
nat.mu.mean <- mu.mean*sd(df$weight) + mean(df$weight)
nat.mu.PI <- mu.PI*sd(df$weight) + mean(df$weight)

# plot raw data
# fading out points to make line and interval more visible
plot( weight ~ height , data=df , col=col.alpha(rangi2,0.5) )

# plot the quap line, aka the mean mu for each weight
lines( nat.H.seq , nat.mu.mean )

# plot a shaded region for 89% PI
shade( nat.mu.PI , nat.H.seq )

# plot the prediction interval (values the model expects)
sim.W <- sim( m2.1 , data=list(H=H.seq) ) 
W.PI <- apply( sim.W , 2 , PI , prob=0.89 )
#convert to natural scale
nat.W.PI <- W.PI*sd(df$weight) + mean(df$weight)

# draw PI region for simulated heights
shade( nat.W.PI , nat.H.seq )



```

Now complete the table

```{r w2.1.4}
#extract mean and 89% predicted values of the posterior distribution to complete the table:
#post<-extract.samples(m4h1)
tabledf <- tabledf %>% mutate(H = (height-mean(df$height))/sd(df$height))
sim_weight<-sim(m2.1, data = tabledf)
tabledf$expected_weight<-apply(sim_weight, 2, mean)*sd(df$weight) + mean(df$weight)
tabledf$lower_interval<-apply(sim_weight, 2, PI, prob = 0.89)[1,]*sd(df$weight) + mean(df$weight)
tabledf$upper_interval<-apply(sim_weight, 2, PI, prob = 0.89)[2,]*sd(df$weight) + mean(df$weight)

kable(tabledf %>% select(-"H"))

```


### Q2  
From the Howell1 dataset, consider only the people younger than 13 years
old. Estimate the causal association between age and weight. Assume that
age influences weight through two paths. First, age influences height, and
height influences weight. Second, age directly influences weight through age-
related changes in muscle growth and body proportions. All of this implies
this causal model (DAG):

```{r w2.2.dag, fig.dim=c(2,1.5)}

g <- dagitty::dagitty( "dag{
  A -> H; A -> W; H -> W
}"
)
dagitty::coordinates(g) <- list(x = c(A = 1, H = 2, W = 3), y = c(A = 1, H = -2, W = 1))
drawdag(g, cex = 1.5, lwd = 3)
```

Use a linear regression to estimate the total (not just direct) causal effect of
each year of growth on weight. Be sure to carefully consider the priors. Try
using prior predictive simulation to assess what they imply.

```{r w2.2}
#load the data and define the model

data("Howell1")
df_kids<-Howell1 %>% filter(age<13)
mean_age = mean(df_kids$age)

#For the total effect of age on weight, no need to include height in the model

m2.2 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b*age,
    a ~ dnorm(3, 1), #mean birth weight is roughly 3kg (wiki) with a small variation
    b ~ dlnorm(0, 1), #weight (typically) increases with age so b must be +ve (still fuzzy on lognorm values)
    sigma ~ dexp(1)
  ), data = df_kids
)

```

Use prior predictive simulation to assess the priors.

```{r w2.2.2}
prior <- extract.prior( m2.2 )
xseq <- 0:12
mu <- link( m2.2 , post=prior , data=list(age = 0:12) )
plot( NULL , xlim=c(0, 12) , ylim=c(0, 30), xlab = 'age', ylab = 'weight' )
for ( i in 1:50 ) lines( 0:12 , mu[i,] , col=col.alpha("black",0.3) )

```

This allows a wide range of relationships between age and weight - some are unreasonably strong, but the data should be able to deal with this.

Let's look at the posterior distribution and model predictions:

```{r w2.2.3}
# define sequence of ages to compute predictions for
# these values will be on the horizontal axis
age_seq <- 0:12

# use link to compute mu
# for each sample from posterior
# and for each height in H.seq
mu <- link( m2.2 , data=data.frame(age=age_seq) )

# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )


# plot raw data
# fading out points to make line and interval more visible
plot( weight ~ age , data=df_kids , col=col.alpha(rangi2,0.5) )

# plot the quap line, aka the mean mu for each weight
lines( age_seq , mu.mean )

# plot a shaded region for 89% PI
shade( mu.PI , age_seq )

# plot the prediction interval (values the model expects)
sim_w <- sim( m2.2 , data=list(age = age_seq) ) 
w_pi <- apply( sim_w , 2 , PI , prob=0.89 )

# draw PI region for simulated heights
shade( w_pi , age_seq )

#plot the coefficient estimates
#coeftab_plot(coeftab(m2.2))

precis(m2.2)

```

Mean birth weight is expected to be about 7kg, with weight increasing by 1.3-1.5kg per year.


### Q3.  

Now suppose the causal association between age and weight might be dif-
ferent for boys and girls. Use a single linear regression, with a categorical
variable for sex, to estimate the total causal effect of age on weight separately
for boys and girls. How do girls and boys differ? Provide one or more pos-
terior contrasts as a summary.

```{r w2.3}
#create sex column in df, where female == 1 and male == 2
df_kids <- df_kids %>% mutate(sex = male+1)

#define the model

m2.3 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma),
    mu <- a[sex] + b[sex]*age,
    a[sex] ~ dnorm(3, 1),
    b[sex] ~ dlnorm(0, 1),
    sigma ~ dexp(1)
    ),
  data = df_kids
)

#print model coeficients
precis(m2.3, depth = 2)

#calculate posterior contrasts
post <- extract.samples(m2.3)
diff_sex <- post$a[,1] - post$a[,2]
PI( diff_sex )

#plot contrasts
age_seq <- 0:12
# compute mu over samples, fixing sex at female
mu_f <- link( m2.3, data=data.frame( sex=1 , age=age_seq ) )

# compute mu over samples, fixing sex at male
mu_m <- link( m2.3, data=data.frame( sex=2 ,  age=age_seq ) )

# summarize to means and intervals
mu_f_mu <- apply( mu_f , 2 , mean )
mu_f_ci <- apply( mu_f , 2 , PI , prob=0.97 )
mu_m_mu <- apply( mu_m , 2 , mean )
mu_m_ci <- apply( mu_m , 2 , PI , prob=0.97 )

#plot the data
cols <- c("red", "blue")
plot(weight ~ age, data = df_kids, col = cols[sex])

# plot the quap line, aka the mean mu for each weight
lines( age_seq , mu_f_mu, col = cols[1], lwd = 3 )
lines( age_seq , mu_m_mu, col = cols[2], lwd = 3 )

# plot a shaded region for 89% PI
shade( mu_f_ci , age_seq, col = col.alpha("red", 0.5))
shade( mu_m_ci , age_seq, col = col.alpha("blue", 0.5))

legend("topleft", c("female", "male"),
       lty = c(1,1),
       col = c("red", "blue"))

```

Boys are typically heavier than girls of the same age, and it looks like the difference increases with age.

BUt let's contrast the predicted distribution of weights across all ages to get a better idea of what's going on.

```{r w2.3.3}
# contrast at each age
Aseq <- 0:12
mu1 <- sim(m2.3,data=list(age=Aseq,sex=rep(1,13)))
mu2 <- sim(m2.3,data=list(age=Aseq,sex=rep(2,13)))
mu_contrast <- mu1
for ( i in 1:13 ) mu_contrast[,i] <- mu2[,i] - mu1[,i]
plot( NULL , xlim=c(0,13) , ylim=c(-15,15) , xlab="age" ,
ylab="girls heavier <- weight difference -> boys heavier" )
for ( p in c(0.5,0.67,0.89,0.99) )
shade( apply(mu_contrast,2,PI,prob=p) , Aseq )
abline(h=0,lty=2,lwd=2)

```

Boys tend to be heavier across all ages, but the distributions overlap a lot, particularly for younger kids. In older kids, the difference between boys' and girls' weight gets stronger.


### Q4  OPTIONAL CHALLENGE. 
The data in data(Oxboys) (rethinking
package) are growth records for 26 boys measured over 9 periods. I want
you to model their growth. Specifically, model the increments in growth
from one period (Occasion in the data table) to the next. Each increment is
simply the difference between height in one occasion and height in the pre-
vious occasion. Since none of these boys shrunk during the study, all of the
growth increments are greater than zero. Estimate the posterior distribution
of these increments. Constrain the distribution so it is always positive—it
should not be possible for the model to think that boys can shrink from year
to year. Finally compute the posterior distribution of the total growth over
all 9 occasions.



```{r w2.4}
data("Oxboys")

#as we haven't been taught how to use random effects in a quap model yet, 
#I presume we ignore the random effect of ID?

#calculate growth increments in df
df <- Oxboys %>% group_by(Subject) %>%
  mutate(increment = case_when(
    Occasion==1 ~ 0,
    TRUE ~ height-lag(height)
  )) %>% ungroup() %>%
  filter(Occasion > 1)  #no need to model increment at Occasion 1 as this is always 0

#define the model:
m2.4 <- quap(
  flist = alist(
    increment ~ dlnorm(alpha, sigma), #use lognormal dist to constrain increments to be positive
    alpha ~ dnorm(0, 0.1), # alpha is the mean of the normal distribution, whose log we use for increment
    sigma ~ dexp(3)
  ),
  data = df
)

#plot prior predictive distribution of increments
prior <- extract.prior( m2.4, n = 1e3 )
sim_increment <- rlnorm(prior$alpha, prior$sigma)
dens(sim_increment)
#The code below also does the same thing (I think)
# simulation from priors
n <- 1e3
alpha <- rnorm(n,0,0.1)
sigma <- rexp(n,3)
delta_sim <- rlnorm(n,alpha,sigma)
dens(delta_sim)
#Play around with values for alpha and sigma - difficult to understand how they will affect the distribution!


#Now plot the distribution of increments from the posterior
post <- extract.samples(m2.4)
dsim <- rlnorm(1e3, post$alpha,post$sigma)
dens(dsim)

#and to get the distribution of total growth over the 9 occasions:
#(what is this doing?
#we're generating 8 values from a lognormal distribution with the same values of alpha and sigma,
#then summing them, and we're doing that for the first 1000 rows from the posterior sample)
inc_sum <- sapply( 1:1000 ,
                   function(s) sum(rlnorm(8,post$alpha[s],post$sigma[s])) )
dens(inc_sum)

```



## Week 3 homework
Covering chapters 5 and 6 of the book (lectures 5 and 6).


### Q1.  
The first two problems are based on the same data. The data in data(foxes)
are 116 foxes from 30 different urban groups in England. These fox groups
are like street gangs. Group size (groupsize) varies from 2 to 8 individuals.
Each group maintains its own (almost exclusive) urban territory. Some ter-
ritories are larger than others. The area variable encodes this information.
Some territories also have more avgfood than others. And food influences
the weight of each fox. Assume this DAG (where F is avgfood, G is groupsize, 
A is area, and W is weight).
Use the backdoor criterion and estimate the total causal influence of A on
F. What effect would increasing the area of a territory have on the amount
of food inside it?:

```{r w3.dag, fig.dim = c(2, 1.5)}
foxdag <- dagitty::dagitty(
  "dag{A -> Fd; Fd -> G; Fd -> W; G -> W}"
)
coordinates(foxdag) <- list(x = c(Fd = 1, A = 2, W = 2, G = 3), y = c(A = -1, Fd = 0, W = 1, G = 0))
drawdag(foxdag, cex = 1.5, lwd = 3)
```

_There are no backdoors between A and F, so just model F as a function of A for the total causal influence._

```{r w3.1}
data("foxes")
#head(foxes)
#str(foxes)

#standardise avgfood and area
foxes<-foxes %>% mutate(A = standardize(area), Fd = standardize(avgfood))

#define the model
m3.1 <- quap(
  flist = alist(
    Fd ~ dnorm(mu, sigma),
    mu <- a + b*A,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = foxes
)

#plot prior predictive distribution:
prior <- extract.prior(m3.1)
xseq <- -3:3
mu <- link( m3.1 , post=prior , data=list(A = xseq) )
plot( NULL , xlim=c(-3,3) , ylim=c(-3,3), xlab = 'std.area', ylab = 'std.food' )
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )

#plot the posterior predictive distribution
# compute mu over samples
mu <- link( m3.1, data=list( A = xseq ) )

# summarize to means and intervals
mu_mean <- apply( mu , 2 , mean )
mu_ci <- apply( mu , 2 , PI , prob=0.97 )

#plot the data
plot(Fd ~ A, data = foxes, xlab = "std.area", ylab = "std.avgfood")

# plot the quap line, aka the mean mu for each weight
lines( xseq , mu_mean, lwd = 3 )

# plot a shaded region for 89% PI
shade( mu_ci , xseq)

# plot the prediction interval (values the model expects)
sim.Fd <- sim( m3.1 , data=list(A = xseq) ) 
Fd.PI <- apply( sim.Fd , 2 , PI , prob=0.89 )
shade( Fd.PI , xseq)

coeftab_plot(coeftab(m3.1))
precis(m3.1)

```

Average food increases with area - for each unit increase in standardised area, standardised avgfood is expected to increase by 0.88 in average.

### Q2.
Now infer both the total and direct causal effects of adding food F to a
territory on the weight W of foxes. Which covariates do you need to adjust for
in each case? In light of your estimates from this problem and the previous
one, what do you think is going on with these foxes? Feel free to speculate—
all that matters is that you justify your speculation.

```{r w3.2}
#For the total causal effect of F on W, no need to control for anything else:

foxes <- foxes %>% mutate(W = standardize(weight))

m3.2total <- quap(
  flist = alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b*Fd,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = foxes
)

#plot posterior predictions
Fseq <- -3:3
mu <- link(m3.2total, data = list(Fd = Fseq))
mu_mean <- apply(mu, 2, mean)
mu_ci <- apply(mu, 2, PI, prob = 0.97)
plot(W ~ Fd, data = foxes, xlab = "std.avgfood", ylab = "std.weight", main = "total effect of average food on weight")
lines(Fseq, mu_mean)
shade(mu_ci, Fseq)
Wsim <- sim(m3.2total, data = list(Fd = Fseq))
sim_PI <- apply(Wsim, 2, PI, prob = 0.89)
shade(sim_PI, Fseq)

```

Looks like no (or very little) total effect of average food on weight.

```{r w3.2.2}
#For the direct causal effect of F on W, must control for G

foxes <- foxes %>% mutate(G = standardize(groupsize))

m3.2direct <- quap(
  flist = alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bF*Fd + bG*G,
    a ~ dnorm(0, 1),
    bF ~ dnorm(0, 1),
    bG ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = foxes
)

#plot posterior predictions (with G set to 0)
Fseq <- -3:3
mu <- link(m3.2direct, data = list(Fd = Fseq, G = 0))
mu_mean <- apply(mu, 2, mean)
mu_ci <- apply(mu, 2, PI, prob = 0.97)
plot(W ~ Fd, data = foxes, xlab = "std.avgfood", ylab = "std.weight", main = "direct effect of food on weight (controlling for group size)")
lines(Fseq, mu_mean)
shade(mu_ci, Fseq)
Wsim <- sim(m3.2direct, data = list(Fd = Fseq, G = 0))
sim_PI <- apply(Wsim, 2, PI, prob = 0.89)
shade(sim_PI, Fseq)

```

Controlling for groupsize, we can see that the direct effect of changing the amount of food available is that weight increases.
These results suggest that areas with more food tend to have bigger groups, but the average weight of wolves in smaller or larger groups is similar (perhaps because of a limit on groupsize imposed by the amount of food available).
If we could hold the groupsize constant but add more food, we would expect the average weight to increase.


### Q3.  
Reconsider the Table 2 Fallacy example (from Lecture 6), this time with an
unobserved confound U that influences both smoking S and stroke Y. Here’s
the modified DAG:

```{r w3.3dag, fig.dim = c(2, 1.5)}
dag3.3<-dagitty("dag{Y <- U -> S; A -> S; A -> X; A -> Y; S -> X; S -> Y; X -> Y}")

coordinates(dag3.3) <- list(x = c(U = 2),
                            y = c(U = -3))

drawdag(dag3.3, cex = 1.5, lwd = 3)

```

First use the backdoor criterion to determine an adjustment set that allows
you to estimate the causal effect of X on Y, i.e. P(Y|do(X)). Second explain
the proper interpretation of each coefficient implied by the regression model
that corresponds to the adjustment set. Which coefficients (slopes) are causal
and which are not? There is no need to fit any models. Just think through
the implications.  

_*answer*_  
_Without the unobserved variable, U, then there are backdoors from X to Y through A and S, so conditioning on A and S should be enough to close both backdoors, using the model Y ~ X + A + S._  
_The unobserved variable U introduces a collider, where S is influenced by A and U, and also a fork, where U influeces both S and Y. However this doesn't influence the effect of X on Y, so we still condition on both A and S, with the same model as above._  

_Check this with dagitty_

```{r w3.4.adjustmentsets}

adjustmentSets(dag3.3, exposure = "X", outcome = "Y")

```

_From the solution pdf:_  
The coefficient for X should still
be the estimate of the causal effect of X on Y, P(Y|do(X)).  
Now the coefficients for A and S are not
even partial causal effects, because both are biased by the collider through
U. In effect the unobserved confound makes the control coefficients unin-
terpretable even as partial causal effects.
The irony here is that is still possible to estimate the casual effect of age A
on Y. But in the model that stratifies by S, the coefficient for age becomes
confounded. It really is not safe to interpret control coefficients, unless there
is an explicit causal model.

### Q4. OPTIONAL CHALLENGE. 
Write a synthetic data simulation for the causal
model shown in Problem 3. Be sure to include the unobserved confound
in the simulation. Choose any functional relationships that you like—you
don’t have to get the epidemiology correct. You just need to honor the causal
structure. Then design a regression model to estimate the influence of X on
Y and use it on your synthetic data. How large of a sample do you need to
reliably estimate P(Y|do(X))? Define “reliably” as you like, but justify your
definition.

```{r 3.4}
n <- 100
bX <- 0
A <- rnorm(n, 0, 0.1)
U <- rnorm(n, 0, 0.1)
S <- rnorm(n, A+U, 0.1)
X <- rnorm(n, A+S, 0.1)
Y <- rnorm(n, U+A+S+bX*X, 0.1)

plot(X, Y)

df <- data.frame (A = A, U = U, S = S, X = X, Y = Y)

m3.4 <- quap(
  flist = alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bA*A + bS*S + bX*X,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    bS ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), 
  data = df
)

precis(m3.4)


```


## Week 4 homework
Covering chapters 7, 8 and 9 of the book (lectures 7 and 8).  

You can use MCMC to solve these problems, if you like. But it’s not re-
quired.  

### Q1. 
Revisit the marriage, age, and happiness collider bias example from Chap-
ter 6. Run models m6.9 and m6.10 again (pages 178–179). Compare these
two models using both PSIS and WAIC. Which model is expected to make
better predictions, according to these criteria? On the basis of the causal
model, how should you interpret the parameter estimates from the model
preferred by PSIS and WAIC?

```{r 4.1}
d <- sim_happiness( seed=1977 , N_years=1000 )
precis(d)

d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

d2$mid <- d2$married + 1 #convert marriage status 0/1 to 1/2

m6.9 <- quap(
  alist(
    happiness ~ dnorm( mu , sigma ),
    mu <- a[mid] + bA*A,
    a[mid] ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
) , data=d2 )
precis(m6.9,depth=2)

m6.10 <- quap( 
  alist(
    happiness ~ dnorm( mu , sigma ),
    mu <- a + bA*A,
    a ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
) , data=d2 )
precis(m6.10)
```

```{r set compareIC plotting method, include = F}
setMethod("plot" , "compareIC" , function(x,y,xlim,SE=TRUE,dSE=TRUE,weights=FALSE,...) {
dev_in <- x[[1]] - x[[5]] # criterion - penalty2
dev_out <- x[[1]]
if ( !is.null(x[['SE']]) ) devSE <- x[['SE']]
dev_out_lower <- dev_out - devSE
dev_out_upper <- dev_out + devSE
if ( weights==TRUE ) {
dev_in <- ICweights(dev_in)
dev_out <- ICweights(dev_out)
dev_out_lower <- ICweights(dev_out_lower)
dev_out_upper <- ICweights(dev_out_upper)
}
n <- length(dev_in)
if ( missing(xlim) ) {
xlim <- c(min(dev_in),max(dev_out))
if ( SE==TRUE & !is.null(x[['SE']]) ) {
xlim[1] <- min(dev_in,dev_out_lower)
xlim[2] <- max(dev_out_upper)
}
}
main <- colnames(x)[1]
set_nice_margins()
dotchart( dev_in[n:1] , labels=rownames(x)[n:1] , xlab="deviance" , pch=16 , xlim=xlim , ... )
points( dev_out[n:1] , 1:n )
mtext(main)
# standard errors
if ( !is.null(x[['SE']]) & SE==TRUE ) {
for ( i in 1:n ) {
lines( c(dev_out_lower[i],dev_out_upper[i]) , rep(n+1-i,2) , lwd=0.75 )
}
}
if ( !all(is.na(x@dSE)) & dSE==TRUE ) {
# plot differences and stderr of differences
dcol <- col.alpha("black",0.5)
abline( v=dev_out[1] , lwd=0.5 , col=dcol )
diff_dev_lower <- dev_out - x$dSE
diff_dev_upper <- dev_out + x$dSE
if ( weights==TRUE ) {
diff_dev_lower <- ICweights(diff_dev_lower)
diff_dev_upper <- ICweights(diff_dev_upper)
}
for ( i in 2:n ) {
points( dev_out[i] , n+2-i-0.5 , cex=0.5 , pch=2 , col=dcol )
lines( c(diff_dev_lower[i],diff_dev_upper[i]) , rep(n+2-i-0.5,2) , lwd=0.5 , col=dcol )
}
}
})

```

```{r plot 4.1}
#compare the models with PSIS and WAIC
#First PSIS
compPSIS <- compare(m6.9, m6.10, func = PSIS)
compPSIS
plot(compPSIS)

#Next WAIC
compWAIC <- compare(m6.9, m6.10, func = WAIC)
compWAIC
plot(compWAIC)

```

Both the PSIS and WAIC indicate that model 6.9 makes better (in- and out-of-sample) predictions.  
PSIS suggests that m6.9 is on average `r compPSIS$dPSIS[2]` units of deviance smaller than m6.10 (with a 99% prediction interval, z score around 2.6, in the range: `r compPSIS$dPSIS[2] + c(-1,1)*compPSIS$dSE[2]*2.6`).  
WAIC also suggests m6.9 is on average `r compWAIC$dWAIC[2]` units of deviance smaller than m6.10 (99% prediction interval: `r compWAIC$dWAIC[2] + c(-1,1)*compWAIC$dSE[2]*2.6`).  


The model preferred by PSIS and WAIC suggests that conditioning on both age and marriage gives better predictions of happiness, contrary to the causal model, in which age and happiness are independent of each other, but have a collider in marriage (A -> M <- H). The parameter estimates from m6.9 suggest that married people are happier than unmarried people, and that happiness decreases with age (but we know from the causal model that this is not true).Knowing the ground truth, these parameter estimates reflect the fact that happier people are more likely to get married earlier, so if we just look at older married people, the average happiness is lower than for younger married people because less happy people are joining the married cohort later.  



### Q2. 
Reconsider the urban fox analysis from last week’s homework. On the
basis of PSIS and WAIC scores, which combination of variables best predicts
body weight (W, weight)? How would you interpret the estimates from the
best scoring model?  

```{r 4.2}
#Reload the foxes data and models from last week

data("foxes")

#standardise avgfood and area
foxes<-foxes %>% mutate(A = standardize(area), Fd = standardize(avgfood))

#For the total causal effect of F on W, no need to control for anything else:
foxes <- foxes %>% mutate(W = standardize(weight))

m3.2total <- quap(
  flist = alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b*Fd,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = foxes
)


#For the direct causal effect of F on W, must control for G
foxes <- foxes %>% mutate(G = standardize(groupsize))

m3.2direct <- quap(
  flist = alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bF*Fd + bG*G,
    a ~ dnorm(0, 1),
    bF ~ dnorm(0, 1),
    bG ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = foxes
)


#Compare the models with PSIS
foxPSIS<-compare(m3.2direct, m3.2total, func = PSIS)
foxPSIS
plot(foxPSIS)

#Compare the models with PSIS
foxWAIC<-compare(m3.2direct, m3.2total, func = WAIC)
foxWAIC
plot(foxWAIC)
```

Again, PSIS and WAIC are similar, both preferring the model of the direct causal influence of F on W, but the difference between the two models' predictive accuracy is small with intervals that overlap (PSIS: mean `r foxPSIS$dPSIS[2]` units of deviance between the direct and total models (99% interval: `r foxPSIS$dPSIS[2]+c(-1,1)*foxPSIS$dSE[2]*2.6`); WAIC: mean `r foxWAIC$dPSIS[2]` units of deviance between the two models(99% interval: `r foxWAIC$dWAIC[2]+c(-1,1)*foxWAIC$dSE[2]*2.6`)).  

### Q3. 
Build a predictive model of the relationship shown on the cover of the book,
the relationship between the timing of cherry blossoms and March temper-
ature in the same year. The data are found in data(cherry_blossoms).
Consider at least two functions to predict doy with temp. Compare them
with PSIS or WAIC.
Suppose March temperatures reach 9 degrees by the year 2050. What does
your best model predict for the predictive distribution of the day-in-year that
the cherry trees will blossom?

```{r 4.3}
data("cherry_blossoms")
#remove years where we don't know the temp
d<-cherry_blossoms %>% filter(!is.na(temp), !is.na(doy))
mean_temp<-mean(d$temp)
d$t<-d$temp - mean_temp

#model doy (day of year of first blossom) as a function of temp centred around the mean (using mHMC)
f<-alist(
  doy ~ dnorm(mu, sigma),
  mu <- a + bt*t,
  a ~ dnorm(90, 5), #assume the average blossom is around 3 months into the year
  bt ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
)

m4.1 <- ulam(
  f, data = d, cores = 3, chains = 3, log_lik = T  #set log_lik to TRUE to compare with WAIC & PSIS
)

#diagnostic plots
traceplot_ulam(m4.1, pars = c("a", "bt", "sigma"))
trankplot(m4.1, pars = c("a", "bt", "sigma"))

#output table
precis(m4.1)


#Second model
#doy as a function of temp (raw values)?
f<-alist(
  doy ~ dnorm(mu, sigma),
  mu <- a + bt*temp,
  a ~ dnorm(90, 5), #assume the average blossom is around 3 months into the year
  bt ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
)

m4.2 <- ulam(
  f, data = d, cores = 3, chains = 3, log_lik = T  #set log_lik to TRUE to compare with WAIC & PSIS
)

#diagnostic plots
traceplot_ulam(m4.2, pars = c("a", "bt", "sigma"))
trankplot(m4.2, pars = c("a", "bt", "sigma"))

#output table
precis(m4.2)

```

The diagnostic plots for both models look ok, and both predict earlier blossoms with higher temps. But which one is a better predictor of the data?  

```{r 4.3 compare}
blossomPSIS<-rethinking::compare(m4.1, m4.2, func = PSIS)
blossomPSIS
plot(blossomPSIS)

blossomWAIC<-rethinking::compare(m4.1, m4.2, func = WAIC)
blossomWAIC
plot(blossomWAIC)

```

Both PSIS and WAIC prefer the first model (with temperature centred around the mean), although the difference between the models is only within a 99% range of `r blossomPSIS$dPSIS[2]-blossomPSIS$dSE[2]*2.6` to `r blossomPSIS$dPSIS[2]+blossomPSIS$dSE[2]*2.6` units of deviance.  

```{r 4.3 predictions}
#predict the day of the first blossom if temperatures reach 9C in March, using m4.1
post <- extract.samples(m4.1)
mu9deg <- post$a + post$bt*(9-mean_temp)

dens( mu9deg , col=rangi2 , lwd=2 , xlab="mu|temp = 9C" ) 
preddate<-PI( mu9deg , prob=0.89 )

avg_dates<-c(round(precis(m4.1)[1, 3]), round(precis(m4.1)[1, 4]))

```

The m4.1 predicted date of first blossom with average March temperatures (`r round(mean_temp,1)`C) is between `r strptime(avg_dates[1], format = "%j") %>% format("%d %b")` and `r strptime(avg_dates[2], format = "%j") %>% format("%d %b")`.  
With March temperatures at 9C, the model predicts the first blossom will appear sometime between `r strptime(preddate[[1]], format = "%j") %>% format("%d %b")` and `r strptime(preddate[[2]], format = "%j") %>% format("%d %b")` (89% prediction interval).  


### Q4-OPTIONAL CHALLENGE.  
The data in data(Dinosaurs) are body mass
estimates at different estimated ages for six different dinosaur species. See
?Dinosaurs for more details. Choose one or more of these species (at least
one, but as many as you like) and model its growth. To be precise: Make
a predictive model of body mass using age as a predictor. Consider two or
more model types for the function relating age to body mass and score each
using PSIS and WAIC.
Which model do you think is best, on predictive grounds? On scientific
grounds? If your answers to these questions differ, why?
This is a challenging exercise, because the data are so scarce. But it is also a
realistic example, because people publish Nature papers with even less data.
So do your best, and I look forward to seeing your growth curves.  


```{r 4.4 optional}
data(Dinosaurs)
with(Dinosaurs, table(species))

#let's go with Massospondylus carinatus as it has the most datapoints
d <- Dinosaurs %>% filter(species=="Massospondylus carinatus") 

#first, let's normalize the mass for easier modelling
d$M <- d$mass/max(d$mass)

#plot data:
d %>% 
  ggplot(aes(x = age, y = M)) +
  geom_point()

f <- alist(
  M ~ dnorm(mu, sigma),
  mu <- a + bA*age,
  a ~ dnorm(0, 0.1),
  bA ~ dnorm(0, 0.1),
  sigma ~ dexp(1)
)

m4.4a <- ulam(flist = f, data = d, chains = 3, cores = 3, log_lik = TRUE)

traceplot_ulam(m4.4a, pars = c("a", "bA", "sigma"))
trankplot(m4.4a, pars = c("a", "bA", "sigma"))
precis(m4.4a)

#plot predicted line from model
aseq <- seq(0,15)
mu4.4 <- link(m4.4a, data = list(age = aseq))
mu_mean <- apply(mu4.4, 2, mean)
mu_ci <- apply(mu4.4, 2, PI, prob = 0.89)
# sim4.4 <- sim(m4.4a, data = list(age = aseq))
# sim_ci <- apply(sim4.4, 2, PI, prob = 0.99)
 pred <- data.frame(age = aseq, mu = mu_mean, ci_l = mu_ci[1,], ci_h = mu_ci[2,])

d %>% 
  ggplot(aes(x = age, y = M)) +
  #put the prediction interval ribbon next as there's an issue with using coord_cartesian
  #and a geom with alpha in R4.1.0+ (see https://github.com/tidyverse/ggplot2/issues/4498)
  #geom_ribbon(data = pred, aes(x = age, y = NULL, ymin = sim_l, ymax = sim_h), fill = "grey") +
  geom_point() + 
  geom_line(data = pred, aes(x = age, y = mu)) +
  geom_ribbon(data = pred, aes(x = age, y = NULL, ymin = ci_l, ymax = ci_h), alpha = 0.5)

```

SO this doesn't look great.  
The growth curve maybe better fit as a logarithmic function. Let's try that. 

```{r 4.4 optional 2}

#plot data with log(mass)
d %>% 
  ggplot(aes(x = age, y = log(M))) +
  geom_point()

f <- alist(
  log(M) ~ dnorm(mu, sigma),
  mu <- a + bA*age,
  a ~ dnorm(0, 0.1),
  bA ~ dnorm(0, 0.1),
  sigma ~ dexp(1)
)

m4.4b <- ulam(flist = f, data = d, chains = 3, cores = 3, log_lik = TRUE)

traceplot_ulam(m4.4a, pars = c("a", "bA", "sigma"))
trankplot(m4.4a, pars = c("a", "bA", "sigma"))
precis(m4.4a)

#plot predicted line from model
aseq <- seq(0,15)
mu4.4 <- link(m4.4b, data = list(age = aseq))
mu_mean <- apply(mu4.4, 2, mean)
mu_ci <- apply(mu4.4, 2, PI)
#sim4.4 <- sim(m4.4b, data = list(age = aseq))
#sim_ci <- apply(sim4.4, 2, PI)
pred <- data.frame(age = aseq, mu = mu_mean, ci_l = mu_ci[1,], ci_h = mu_ci[2,])

d %>% 
  ggplot(aes(x = age, y = M)) +
  #put the prediction interval ribbon next as there's an issue with using coord_cartesian
  #and a geom with alpha in R4.1.0+ (see https://github.com/tidyverse/ggplot2/issues/4498)
  #geom_ribbon(data = pred, aes(x = age, y = NULL, ymin = sim_l, ymax = sim_h), fill = "grey") +
  geom_point() + 
  geom_line(data = pred, aes(x = age, y = mu)) +
  geom_ribbon(data = pred, aes(x = age, y = NULL, ymin = ci_l, ymax = ci_h), alpha = 0.5)

```

Neither model looks great.
Let's compare them with PSIS and WAIC.

```{r 4.4 optional compare}
PSIS4<-compare(m4.4a, m4.4b, func = "PSIS")
WAIC4<-compare(m4.4a, m4.4b, func = "WAIC")

plot(PSIS4)

PSIS4
WAIC4

```

PSIS and WAIC give almost identical results. They very slightly prefer the model predicting the mass (which is between `r PSIS4$dPSIS[2]-(PSIS4$dSE[2]*2.6)` and `r PSIS4$dPSIS[2]+(PSIS4$dSE[2]*2.6)` units of deviance smaller than the model with log(mass) (99% interval)).

From the solutions, McElreath uses a biological model of growth (adapted from the Bertalanffy growth model), like this: 

```{r 4 optional solutions}

f <- alist(
  M ~ normal(mu, sigma),
  mu <- k*(1-exp(-b*age))^a,
  a ~ exponential(0.1),
  b ~ exponential(1),
  k ~ normal(1, 0.5),
  sigma ~ exponential(1)
)

m4.4c <- ulam(flist = f, data = d, chains = 3, log_lik = TRUE)

traceplot(m4.4c, pars = c("b", "k", "sigma"))
trankplot(m4.4c, pars = c("b", "k", "sigma"))
precis(m4.4c)

mu4.4c <- link(m4.4c, data = list(age = aseq))
mu_mean <- apply(mu4.4c, 2, mean)
mu_ci <- apply(mu4.4c, 2, PI)
pred<-data.frame(age = aseq, mu = mu_mean, ci_l = mu_ci[1,], ci_u = mu_ci[2,])


ggplot(d, aes(x = age, y = M)) + 
  geom_point() +
  geom_line(data = pred, aes(x = age, y = mu_mean)) +
  geom_ribbon(data = pred, aes(x = age, y = NULL, ymin = ci_l, ymax = ci_u), alpha = 0.5)

```

Much better!